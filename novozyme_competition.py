# -*- coding: utf-8 -*-
"""novozyme_competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SOg63JnOJkQ-e-1oFlQeCaDvnmIlJhBX

# The Challenge

Take in a dataset of enzyme amino acid sequences and determine the thermal stability of them.

**Features:** 
- Amino acid sequence
- pH
- Data source (will not include in model as not useful)

**Target:**
- thermostability of enzyme variants based on experimental melting temperature data

**Initial Strategy:** 
- Update the data with the corrected dataset Kaggle provided
- Separate aa sequences into separate characters 
- Prepare a `TextVectorizer` for the train characters
- Create an `Embedding` layer for the vectorized chars 
Create `tf.data.Dataset` performant datasets for training and validation sets
- Train on only 10% of the data to find what works 
- Find what works and then scale up to 100% of the training data
"""

!nvidia-smi -L

import tensorflow as tf 
print(tf.__version__)

# Get the data
from google.colab import drive
drive.mount('/content/drive')

# Unzip the folder
!unzip "/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/novozymes-enzyme-stability-prediction.zip"

"""## Data Preparation"""

# Read in the CSV files
import pandas as pd 
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")
train_update_df = pd.read_csv("train_updates_20220929.csv")

train_df.head()

len(train_df), len(train_update_df), len(test_df)

test_df.head()

test_df.tail()

# DataFrame as that has rows that need to be taken out and rows that need to be added
train_update_df.head()

import numpy as np

# Indices for rows that should be added
np.where(train_update_df['pH'].notnull() == True)

# Verify
train_update_df.iloc[28]

# Number of rows in train_update_df that need to be added main dataframe should 
# equal 25 per the kaggle website
len(np.where(train_update_df['pH'].notnull() == True)[0])

# dataframe of rows to add to the primary train_df DataFrame
train_update_df.iloc[np.where(train_update_df['pH'].notnull() == True)[0]]

# All rows that need to be taken out of the original train_df 
# These rows in the main train_df dataframe are of bad quality per Kaggle
# Using pH column as proxy to determine which rows should be removed
rows_to_drop = train_update_df.iloc[np.where(train_update_df['pH'].isnull() == True)[0]]
rows_to_drop

len(rows_to_drop)

# Setting the seq_id column to the index 
rows_to_drop.set_index('seq_id', inplace=True)

rows_to_drop.index

# Remove unwanted rows from our training data
main_dataframe = train_df.drop(index=rows_to_drop.index, axis=0)

main_dataframe.head()

len(main_dataframe) # length should be len(train_df) - 2409

# Rows that need to be added to main_dataframe
train_update_df.iloc[np.where(train_update_df['pH'].notnull() == True)[0]].head()

# Rows that had pH and tm transposed (need to be swapped out)
# No pH value should be higher than 14.0
# Use that as proxy to eliminate bad rows
main_dataframe.iloc[np.where(main_dataframe['pH'] > 14.0)[0]]

main_dataframe.iloc[np.where(main_dataframe['pH'] > 14.0)[0]].index

# Dropped the rows that needed to be swapped out 
main_dataframe.drop(index=main_dataframe.iloc[np.where(main_dataframe['pH'] > 14.0)[0]].index, axis=0, inplace=True)
len(main_dataframe)

# Verify correct rows were dropped
np.where(main_dataframe['pH'] > 14.0)[0]

# Adding the corrected rows back to main_dataframe
# Where the pH and tm values were corrected
main_dataframe = pd.concat([main_dataframe, train_update_df.iloc[np.where(train_update_df['pH'].notnull() == True)[0]]])
len(main_dataframe)

# Examine final dataframe for correctness
main_dataframe.sort_values(by=['seq_id'], axis=0, inplace=True)
main_dataframe.head()

# Ensure no NaN values
main_dataframe.dropna(subset=['pH'], inplace=True)
main_dataframe

# Check for NA values 
main_dataframe.isna().any()

"""We have taken away and added the neccessary rows to our training dataframe (`main_dataframe`). We have also dropped any NA values. Now, we can build a performant data pipeline for our data and then begin building the our experimental models to predict thermal stability for the enzymes"""

# Create feature and target dataframes
main_dataframe = main_dataframe.sample(frac=1, random_state=7) # shuffle dataframe
features_df = main_dataframe[['protein_sequence', 'pH']] # Get features
targets_df = main_dataframe[['tm']] # get targets

features_df.head()

targets_df.head()

# Create target vector
import tensorflow as tf
targets = tf.squeeze(targets_df).numpy()
targets[:10]

# Examine test dataframe
test_df.head()

# Make function to split protein sequences into individual characters
# Evaluting protein sequence character by character
# Model will need to read each character separately 
def split_chars(text):
  return " ".join(list(text))

# Test out our split_chars function
[split_chars(sequence) for sequence in features_df['protein_sequence'][:2]]

# Create list of of train protein charified sequences
all_seq_char = [split_chars(sequence) for sequence in list(features_df['protein_sequence'])]
all_seq_char[:2]

len(all_seq_char)

# What's the average character length? Divide by 2 to account for space characters
char_lens = [int(len(sequence)/2) for sequence in all_seq_char]
np.mean(char_lens).round()

max(char_lens)

# Unique characters in protein sequence
set(all_seq_char[0])

# Get max number of different individual characters
max_ = 0
for sequence in all_seq_char:
  if len(set(sequence)) > max_:
    max_ = len(set(sequence))

print(max_)

import tensorflow as tf
from tensorflow.keras.layers import TextVectorization

#train_features['protein_sequence'].apply(split_chars)

# Check distribution of protein sequences at character level
import matplotlib.pyplot as plt 
plt.hist(char_lens, bins=15)

# What character length covers 95% of the sequenes?
output_seq_char_len = int(np.percentile(char_lens, 99))
output_seq_char_len

max_vocab_length =  22 # add one for OOV token
max_sequence_length = output_seq_char_len # length that covers 97% of our protein sequences

char_vectorizer = TextVectorization(max_tokens=max_vocab_length,
                                    output_mode='int',
                                    output_sequence_length=max_sequence_length,
                                    #pad_to_max_tokens=False,
                                    #split='character', #could optionally specify this parameter if we didn't split characters previously
                                    name='char_vectorizer')

np.asarray(all_seq_char[0])

char_vectorizer.adapt(np.asarray(all_seq_char))

char_vectorizer.get_config()

list_ = [split_chars(sequence) for sequence in test_df['protein_sequence']]
list_[0]

list_ = list(test_df['protein_sequence'].values)
list_[0]

letter_dict = {}
for idx, letter in enumerate(list_[2]):
  letter_dict[letter] = list_[2].count(letter)

letter_dict

# Testing char_vectorizer on one sequence (padded with zeros)
import random
idx = random.randint(0, len(features_df))
vectorized_char_sequence = char_vectorizer(split_chars(features_df['protein_sequence'].iloc[idx]))
vectorized_char_sequence.numpy()

vectorized_char_sequence[:500]

# Check padded length
len(vectorized_char_sequence.numpy())

tf.squeeze(char_vectorizer([all_seq_char[idx]])).numpy().max() # max integer value is 21

# Character vocab stats
char_vocab = char_vectorizer.get_vocabulary()
print(f"Number of different characters in character vocab: {len(char_vocab)}")
print(f"Unique letters in the protein sequences:\n {set(list(char_vocab))}")

# Test out character vectorizer
import random
random_train_seq = random.choice(all_seq_char)
print(f"Charified text:\n {random_train_seq}")
print(f"\nLength of random_train_chars: {len(random_train_seq.split(sep=' '))}")
print(f"\nVectorized chars:\n {char_vectorizer(random_train_seq)}")
print(f"\nLength of vectorized chars: {len(char_vectorizer(random_train_seq))}")

max_sequence_length, max_vocab_length

# Creating a character-level embedding layer 
from tensorflow.keras import layers
character_embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set the input shape, number of different characters, max int value is 21
                                       output_dim=256, 
                                       mask_zero=True,
                                       input_length=max_sequence_length)

# Test our character embedding layer
print(f"Charified text:\n {random_train_seq}\n")
print(f"Length of charified text: {round(len(random_train_seq)/2)}\n")
char_embed_example = character_embedding(char_vectorizer([random_train_seq]))
print(f"Embedded chars: (after vectorization and embedding)\n {char_embed_example}\n")
print(f"Shape of character embeddings:\n {char_embed_example.shape}") # [batch_size, input_length, embedding dim]

"""**Idea:** Using `TextVectorization` layer to create and array of counts of each letter in the protein sequences and using that as a source of input for the model"""

max_vocab_length = 20
letter_count_vectorizer = TextVectorization(max_tokens=max_vocab_length,
                                    output_mode='count',
                                    #output_sequence_length=max_sequence_length,
                                    pad_to_max_tokens=False,
                                    #split='character', #could optionally specify this parameter if we didn't split characters previously
                                    name='char_vectorizer')

letter_count_vectorizer.adapt(np.asarray(all_seq_char))

# Testing char_vectorizer on one sequence (padded with zeros)
import random
idx = random.randint(0, len(features_df))
vectorized_char_sequence = letter_count_vectorizer(all_seq_char[idx])
vectorized_char_sequence.numpy()

# Will produce constant size tensor equal to [max tokens] parameter
vectorized_char_sequence.shape

list_ = all_seq_char[idx]
list_

letter_dict = {}
for letter in list_:
  letter_dict[letter] = list_.count(letter)

letter_dict

"""## Modelling Experiments to run
- Conv1D 
- LSTM
- Bidirectional LSTM
- GRU

## Creating performant datasets (training and validation sets)
"""

# Turn our 10% data into TensorFlow Datasets  from the dataframes
# train_features_10_percent = tf.data.Dataset.from_tensor_slices((tf.cast(train_features_10_percent_df['protein_sequence'].values, tf.string),
# tf.cast(train_features_10_percent_df['pH'].values, tf.float32)))
# train_labels_10_percent = tf.data.Dataset.from_tensor_slices(tf.cast(train_targets_10_percent, tf.float32))
# train_dataset_10_percent = tf.data.Dataset.zip((train_features_10_percent, train_labels_10_percent))

# # Take 10% of the data and turn it into TensorFlow Datasets
# val_features = tf.data.Dataset.from_tensor_slices((tf.cast(val_features_df['protein_sequence'].values, tf.string), 
#                                                    val_pH))
# val_labels = tf.data.Dataset.from_tensor_slices(tf.cast(val_targets, tf.float32))
# val_dataset = tf.data.Dataset.zip((val_features, val_labels))

# # Turn test_df into TensorFlow Datasets
# test_data = tf.data.Dataset.from_tensor_slices((tf.cast(test_df['protein_sequence'].values, tf.string),
#                                                    tf.cast(test_df['pH'], tf.float32)))

# train_dataset_10_percent, val_dataset, test_data

test_df[['pH']].max(), test_df[['pH']].min()

test_char_seq = tf.cast([split_chars(sequence) for sequence in test_df['protein_sequence']], tf.string)
test_ph_norm = tf.cast(test_df['pH'].values/11, tf.float32)
test_ph = tf.cast(test_df['pH'].values, tf.float32)
test_char_seq[:2], test_ph_norm[:2]

test_letter_counts = letter_count_vectorizer(test_char_seq)
test_letter_counts = test_letter_counts/max_letter_count
test_letter_counts

test_tribrid_char_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(test_letter_counts, tf.float32),test_char_seq, test_ph_norm))
test_tribrid_char_dataset = test_tribrid_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
test_tribrid_char_dataset

# Turn test_df into TensorFlow Datasets
test_char_sequences = tf.data.Dataset.from_tensor_slices(test_char_seq)
test_ph = tf.data.Dataset.from_tensor_slices(test_ph_norm)
test_char_dataset = tf.data.Dataset.zip((test_char_sequences, test_ph))
test_char_dataset

list(test_char_dataset.as_numpy_iterator())[0]

test_dataset = test_data.batch(32).prefetch(tf.data.AUTOTUNE)
test_dataset

features_df.head()

targets_df.head()

from sklearn.model_selection import train_test_split  

X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(features_df, targets_df, test_size=0.05,
                                                              random_state=7)
len(X_train_df), len(X_val_df), len(y_train_df), len(y_val_df)

X_train_df[['protein_sequence']] = X_train_df[['protein_sequence']].applymap(lambda x: split_chars(x))
X_train_df

y_train_df.head()

X_val_df[['protein_sequence']] = X_val_df[['protein_sequence']].applymap(lambda x: split_chars(x))
X_val_df

y_val_df.head()

tf.squeeze(y_train_df.values)

np.unique(features_df['pH'])

np.unique(test_df['pH'])

features_df['pH'].to_numpy().reshape((-1,1))

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
#scaler = StandardScaler()
#print(scaler.fit_transform(features_df['pH'].to_numpy().reshape((-1,1))))

min_max_scaler = MinMaxScaler()
min_max_scaler.fit(features_df['pH'].to_numpy().reshape((-1,1)))

train_seq_char = tf.cast(X_train_df['protein_sequence'].values, tf.string)
train_ph = tf.cast(min_max_scaler.transform(X_train_df['pH'].values.reshape((-1,1))), tf.float32)
train_targets = tf.cast(tf.squeeze(y_train_df.values), tf.float32)
val_seq_char = tf.cast(X_val_df['protein_sequence'].values, tf.string)
val_ph = tf.cast(min_max_scaler.transform(X_val_df['pH'].values.reshape((-1,1))), tf.float32)
val_targets = tf.cast(tf.squeeze(y_val_df.values), tf.float32)

len(train_seq_char), len(train_ph), len(train_targets), len(val_seq_char), len(val_ph), len(val_targets)

test_char_seq = tf.cast([split_chars(sequence) for sequence in test_df['protein_sequence']], tf.string)
test_ph_norm = min_max_scaler.transform(tf.cast(test_df['pH'].values.reshape((-1,1)), tf.float32))
test_ph = tf.cast(test_df['pH'].values, tf.float32)
test_char_seq[:2], test_ph_norm[:2]

"""### Create charified `tf.data.Dataset`"""

# Create charified tf.data.Dataset with two inputs, seqeunce and ph value
### Train dataset ###
# char_train_features = tf.data.Dataset.from_tensor_slices((train_seq_char, train_ph))
# train_labels = tf.data.Dataset.from_tensor_slices(train_targets)
# train_char_dataset = tf.data.Dataset.zip((char_train_features, train_labels))
# train_char_dataset = train_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


# ### Validation dataset ###
# char_val_features = tf.data.Dataset.from_tensor_slices((val_seq_char, val_ph))
# char_val_labels = tf.data.Dataset.from_tensor_slices(val_targets)
# val_char_dataset = tf.data.Dataset.zip((char_val_features, char_val_labels))
# val_char_dataset = val_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


# train_char_dataset, val_char_dataset

"""### Create Tribrid dataset"""

train_seq_char.numpy()[0].decode()

letter_count_vectorizer(train_seq_char.numpy()[2]) == letter_count_vectorizer(train_seq_char.numpy()[2].decode())

train_letter_counts = letter_count_vectorizer(train_seq_char)
train_letter_counts

val_letter_counts = letter_count_vectorizer(val_seq_char)
val_letter_counts

min_max_scaler_count = MinMaxScaler()
min_max_scaler_count.fit(train_letter_counts)

val_letter_counts.numpy()[0]

max_letter_count = 0
for array in train_letter_counts.numpy():
  if array.max() > max_letter_count:
    max_letter_count = array.max()

print(max_letter_count)

max_ = 0
for array in val_letter_counts.numpy():
  if array.max() > max_:
    max_ = array.max()

print(max_)

train_letter_counts = min_max_scaler_count.transform(train_letter_counts)
val_letter_counts = min_max_scaler_count.transform(val_letter_counts)
train_letter_counts, val_letter_counts

train_letter_counts.shape, val_letter_counts.shape

# Create charified tf.data.Dataset
### Train dataset ###
char_train_features = tf.data.Dataset.from_tensor_slices((tf.cast(train_letter_counts, tf.float32), train_seq_char, train_ph))
train_labels = tf.data.Dataset.from_tensor_slices(train_targets)
train_char_dataset = tf.data.Dataset.zip((char_train_features, train_labels))
tribrid_train_char_dataset = train_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


### Validation dataset ###
char_val_features = tf.data.Dataset.from_tensor_slices((tf.cast(val_letter_counts, tf.float32), val_seq_char, val_ph))
char_val_labels = tf.data.Dataset.from_tensor_slices(val_targets)
val_char_dataset = tf.data.Dataset.zip((char_val_features, char_val_labels))
tribrid_val_char_dataset = val_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


tribrid_train_char_dataset, tribrid_val_char_dataset

### Train dataset ###
char_train_features = tf.data.Dataset.from_tensor_slices((train_seq_char, train_ph))
train_labels = tf.data.Dataset.from_tensor_slices(train_targets)
train_char_dataset = tf.data.Dataset.zip((char_train_features, train_labels))
train_char_dataset = train_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


### Validation dataset ###
char_val_features = tf.data.Dataset.from_tensor_slices((val_seq_char, val_ph))
char_val_labels = tf.data.Dataset.from_tensor_slices(val_targets)
val_char_dataset = tf.data.Dataset.zip((char_val_features, char_val_labels))
val_char_dataset = val_char_dataset.batch(32).prefetch(tf.data.AUTOTUNE)


train_char_dataset, val_char_dataset

# Create function to plot historys
import matplotlib.pyplot as plt

def plot_historys(history):
  """
  A function to plot the loss and accuracy of a Tensorflow model

  Args
  -----
  history : history object from a trained TensorFlow Model 
  """
  # Get variables from history object
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  mae = history.history['mae']
  val_mae = history.history['val_mae']
  epochs = history.epoch

  # Plot the loss and mse 
  fig = plt.figure(figsize=(10,7))
  plt.subplot(1,2,1)
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.legend()
  plt.xlabel("Epochs")
  plt.title("Training Loss vs. Val Loss")

  plt.subplot(1,2,2)
  plt.plot(epochs, mae, label='training_mae')
  plt.plot(epochs, val_mae, label='val_mae')
  plt.legend()
  plt.xlabel("Epochs")
  plt.title("Training MAE vs. Val MAE")

# Setup some callbacks
import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,
                              patience=6, min_delta=1, min_lr=0.00001, verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=12, min_delta=0.5,
                               restore_best_weights=True)

import os 
os.mkdir("model_experiments/")

checkpoint_dir = "/content/model_experiments"

# Create ModelCheckpoint callback to save our model's progress during training 
def model_checkpoint(model_name:str):
  model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir + "/" + model_name,
                                                        monitor='val_loss',
                                                        save_best_only=True,
                                                        save_weights_only=True,
                                                        verbose=0)
  return model_checkpoint

import datetime

def create_tensorboard_callback(dir_name, experiment_name):
  """
  Creates a TensorBoard callback instand to store log files.

  Stores log files with the filepath:
    "dir_name/experiment_name/current_datetime/"

  Args:
    dir_name: target directory to store TensorBoard log files
    experiment_name: name of experiment directory (e.g. efficientnet_model_1)
  """
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback

BATCH_SIZE = 32

SAVE_DIR = "novoenzyme_model_logs"

# 1. Setup protein sequence inputs
# Inputs are 1D strings of characters
sequence_inputs = layers.Input(shape=(1,), batch_size=BATCH_SIZE, dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Conv1D(64, kernel_size = 3, activation='relu')(sequence_embeddings)
x = layers.Conv1D(32, kernel_size=3, activation='relu')(x)
sequence_outputs = layers.GlobalMaxPooling1D()(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), batch_size = BATCH_SIZE, dtype=tf.float32, name='ph_inputs'
pH_outputs = layers.Dense(128, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# Concatenate sequence and pH inputs 
sequence_pH_concat = layers.Concatenate(name='concatenation')([sequence_model.output, 
                                                               pH_model.output])

combined_dense = layers.Dense(128, activation='relu')(sequence_pH_concat)
output_layer = layers.Dense(1, activation='linear')(combined_dense) 

# Construct the model 
model_1 = tf.keras.Model(inputs=[sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_1_Conv1D')

"""### Model 1: Conv1D"""

model_1.summary()

from tensorflow.keras.utils import plot_model
plot_model(model_1)

model_1.compile(loss ='mse', # will use mse for loss as bigger errors should be penlized more given the problem statement 
                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['mae', 'mse']) # want mae too as intuitive with how far off our predicted target numbers are

history_1 = model_1.fit(train_dataset_10_percent, 
                        steps_per_epoch = len(train_dataset_10_percent),
                        epochs=30,
                        validation_data=val_dataset,
                        validation_steps = int(0.25*len(val_dataset)),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_1.name),
                                   create_tensorboard_callback(SAVE_DIR, model_1.name)])

plot_historys(history_1)

model_1.evaluate(val_dataset)

model_1.get_weights()

test_df_shuffled = test_df.sample(frac=1)

model_1_preds = model_1.predict((test_df_shuffled['protein_sequence'].apply(split_chars).values, normalizer1(test_df_shuffled['pH'].values)))

model_1_preds[200:300], model_1_preds.shape

model_1.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_1")

"""## Model_2 LSTM """

# 1. Setup protein sequence inputs
sequence_inputs = layers.Input(shape=(1), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.LSTM(64, return_sequences=True)(sequence_embeddings)
sequence_outputs = layers.LSTM(64)(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
norm1 = normalizer1(pH_inputs)
pH_outputs = layers.Dense(128, activation='relu')(norm1)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# Concatenate sequence and pH inputs 
sequence_pH_concat = layers.Concatenate(name='concatenation')([sequence_model.output, 
                                                               pH_model.output])

combined_dense = layers.Dense(128, activation='relu')(sequence_pH_concat)
final_dense = layers.Dense(128, activation='relu')(combined_dense)
output_layer = layers.Dense(1, activation='linear')(final_dense)

# Construct the model 
model_2 = tf.keras.Model(inputs=[sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_2_LSTM')

model_2.summary()

model_2.compile(loss ='mse',
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_2 = model_2.fit(train_dataset_10_percent, 
                        steps_per_epoch = len(train_dataset_10_percent),
                        epochs=30,
                        validation_data=val_dataset,
                        validation_steps = int(0.25*len(val_dataset)),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_2.name)])

model_2.evaluate(val_dataset)

plot_historys(history_2)

"""## Bidirectional LSTM

- Two input model 
- Tribrid model
"""

# 1. Setup protein sequence inputs
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(128, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)


# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])


# 6. Create output layers
combined_dense = layers.Dense(256, activation='relu')(sequence_ph_concat)
combined_dropout = layers.Dropout(0.5)(combined_dense)
x = layers.Dense(128, activation='relu')(combined_dropout)
final_dropout = layers.Dropout(0.4)(x)
final_dense = layers.Dense(128, activation='relu')(final_dropout)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_3 = tf.keras.Model(inputs=[sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_3_bidirectional_LSTM')

model_3.summary()

model_3.compile(loss='mse',
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['mae'])

model_3.load_weights("/content/drive/MyDrive/model_3_bidirectional_LSTM")

model_3.evaluate(val_char_dataset)

history_3 = model_3.fit(train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(train_char_dataset),
                        validation_data=val_char_dataset,
                        validation_steps = len(val_char_dataset),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_3.name)])

model_3.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_3_bidirectional_lstm_tribrid_from_checkpoint")

history_3 = model_3.fit(train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(train_char_dataset),
                        validation_data=val_char_dataset,
                        validation_steps = len(val_char_dataset),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_3.name)])

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(256, activation='relu')(sequence_ph_concat)
z = layers.Dropout(0.2)(z)
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.2)(z)

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])
# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(z) 

# Construct the model 
model_3 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_3_bidirectional_LSTM_tribrid')

model_3.summary()

# PLot model_3 
from tensorflow.keras.utils import plot_model 
plot_model(model_3, show_shapes=True)

model_3.compile(loss='mse',
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_3 = model_3.fit(tribrid_train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(tribrid_train_char_dataset),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_3.name)])

model_3.evaluate(tribrid_val_char_dataset)

plot_historys(history_3)

model_3_preds = model_3.predict(val_char_dataset)
model_3_preds[:50]

model_3.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_3_bidirectional_lstm_tribrid_letter_count_normalized")

import tensorflow as tf
# Load model 
loaded_model_3 = tf.keras.models.load_model("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_3_bidirectional_lstm_2")

loaded_model_3.evaluate(val_char_dataset)

model_3_preds = model_3.predict([test_letter_counts, test_char_seq, test_ph_norm])
model_3_preds

model_3_preds[:50]

list(tribrid_train_char_dataset.take(1).as_numpy_iterator())

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(256, activation='relu')(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])
# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(z) 

# Construct the model 
model_4 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_4_bidirectional_LSTM_tribrid_with_batch_norm')

model_4.compile(loss='mse',
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_4 = model_4.fit(tribrid_train_char_dataset.take(50), 
                        epochs=10,
                        steps_per_epoch = len(tribrid_train_char_dataset.take(50)),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_4.name)])



# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(256, activation='relu')(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

final_dense = layers.Dense(1024, activation='relu')(z)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_5 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_5_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm')

model_5.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_5 = model_5.fit(tribrid_train_char_dataset.take(50), 
                        epochs=10,
                        steps_per_epoch = len(tribrid_train_char_dataset.take(50)),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset))
                        #callbacks=[reduce_lr, early_stopping, model_checkpoint(model_4.name)])

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(1024, activation='relu')(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(256, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

final_dense = layers.Dense(1024, activation='relu')(z)
x = layers.BatchNormalization()(final_dense)
x = layers.Dropout(0.2)(x)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(x) 

# Construct the model 
model_6 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_6_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm')

model_6.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_6 = model_6.fit(tribrid_train_char_dataset.take(50), 
                        epochs=10,
                        steps_per_epoch = len(tribrid_train_char_dataset.take(50)),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset))

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(1024, activation='relu')(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(256, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(1024, activation='relu')(z)
x = layers.BatchNormalization()(combined_dense)
x = layers.Dropout(0.2)(x)
final_dense = layers.Dense(128, activation='relu')(x)


# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_7 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_7_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm')

model_7.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_7 = model_7.fit(tribrid_train_char_dataset.take(50), 
                        epochs=15,
                        steps_per_epoch = len(tribrid_train_char_dataset.take(50)),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset))

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(1024, activation='relu')(sequence_ph_concat)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(256, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)


# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(1024, activation='relu')(z)
x = layers.BatchNormalization()(combined_dense)
x = layers.Dropout(0.2)(x)
final_dense = layers.Dense(128, activation='relu')(x)


# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_8 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_8_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm')

model_8.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_8 = model_8.fit(tribrid_train_char_dataset.take(50), 
                        epochs=10,
                        steps_per_epoch = len(tribrid_train_char_dataset.take(50)),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset))

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(1024, activation='relu')(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(256, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4
z = layers.Dense(128, activation='relu')(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dense(128, activation='relu')(z)
z = layers.Dropout(0.4)(z) # increase dropout to 0.4

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(1024, activation='relu')(z)
x = layers.BatchNormalization()(combined_dense)
x = layers.Dropout(0.2)(x)
final_dense = layers.Dense(128, activation='relu')(x)


# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_9 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_9_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm')

model_9.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

model_9.load_weights("/content/drive/MyDrive/model_9_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm")

model_9.evaluate(tribrid_val_char_dataset)

history_9 = model_9.fit(tribrid_train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(tribrid_train_char_dataset),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint(model_9.name)])

model_9.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_9_tribrid__final_dense_batch_norm")

model_9_preds = model_9.predict([test_letter_counts, test_char_seq, test_ph_norm])
model_9_preds

model_9_preds[:50]

"""Let's make model_9 less complex and add some more regularization technqiues"""

from tensorflow.keras import regularizers

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(64))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(32, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
x = layers.Dense(32, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=x)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5
z = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5
z = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(z)
x = layers.BatchNormalization()(combined_dense)
x = layers.Dropout(0.4)(x)
final_dense = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)


# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dense) 

# Construct the model 
model_10 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_10_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm_regul')

model_10.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_10 = model_10.fit(tribrid_train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(tribrid_train_char_dataset),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint(model_10.name)])

plot_historys(history_10)

model_10.evaluate(tribrid_val_char_dataset)

model_10_preds = model_10.predict([test_letter_counts, test_char_seq, test_ph_norm])
model_10_preds[:100]

model_10.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_10_tribrid__final_dense_batch_norm")

# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(sequence_embeddings)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(64, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
letter_count_outputs = layers.Dense(64, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=letter_count_outputs)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(512, activation='elu', kernel_regularizer=regularizers.l2(0.001))(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5
z = layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001))(z)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5

# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001))(z)
x = layers.BatchNormalization()(combined_dense)
x = layers.Dropout(0.3)(x)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(x) 

# Construct the model 
model_11 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_11_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm_regul')

model_11.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['mae', 'mse'])

history_11 = model_11.fit(tribrid_train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(tribrid_train_char_dataset),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint(model_11.name)])

model_11.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_11_tribrid__final_dense_batch_norm")

plot_historys(history_11)

model_11.evaluate(tribrid_val_char_dataset)

model_11_preds = model_11.predict([test_letter_counts, test_char_seq, test_ph_norm])
model_11_preds[:50]



# 1. Setup protein sequence inputs (tribrid model)
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(sequence_embeddings)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(64, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# 3. Set up letter count inputs 
letter_count_inputs = layers.Input(shape=(20,), dtype=tf.float32, name="letter_count_input")
letter_count_outputs = layers.Dense(64, activation='relu')(letter_count_inputs)
letter_count_model = tf.keras.Model(inputs=letter_count_inputs, 
                                    outputs=letter_count_outputs)

# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])

z = layers.Dense(512, activation='elu', kernel_regularizer=regularizers.l2(0.002))(sequence_ph_concat)
z = layers.BatchNormalization()(z) # Add in BatchNorm layer
z = layers.Dropout(0.5)(z) # increase dropout to 0.5


# 5. Combine letter count embeddings with sequence and ph embeddings for tribrid model
z = layers.Concatenate(name='sequence_ph_lettercount_embedding')([letter_count_model.output, 
                                                                  z])

combined_dense = layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.002))(z)
x = layers.Dropout(0.3)(combined_dense)
final_dense = layers.Dense(128, activation='elu')(x)
final_dropout = layers.Dropout(0.3)(final_dense)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dropout) 

# Construct the model 
model_12 = tf.keras.Model(inputs=[letter_count_model.input, sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_12_bidirectional_LSTM_tribrid_with_final_dense_plus_batch_norm_regul')

model_12.summary()

model_12.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['mae', 'mse'])

history_12 = model_12.fit(tribrid_train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(tribrid_train_char_dataset),
                        validation_data=tribrid_val_char_dataset,
                        validation_steps = len(tribrid_val_char_dataset),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint(model_12.name)])

model_12.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_12_tribrid__final_dense_batch_norm")

model_12.evaluate(tribrid_val_char_dataset)

model_12_preds = model_12.predict(tribrid_val_char_dataset)
model_12_preds[:50]

# 1. Setup protein sequence inputs
sequence_inputs = layers.Input(shape=(1,), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(sequence_embeddings)
sequence_outputs = layers.Bidirectional(layers.LSTM(128))(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='ph_inputs')
pH_outputs = layers.Dense(128, activation='relu')(pH_inputs)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)


# 4. Concatenate sequence and pH inputs 
sequence_ph_concat = layers.Concatenate(name='concat_sequence_ph')([sequence_model.output, 
                                                                    pH_model.output])


# 6. Create output layers
combined_dense = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(sequence_ph_concat)
combined_dropout = layers.Dropout(0.5)(combined_dense)
x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001))(combined_dropout)
x = layers.Dropout(0.5)(x)
x = layers.Dense(128, activation='relu')(x)
final_dropout = layers.Dropout(0.3)(x)

# 7. Create output layer
output_layer = layers.Dense(1, activation='linear')(final_dropout) 

# Construct the model 
model_13 = tf.keras.Model(inputs=[sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_13_bidirectional_LSTM_with_l2_norm_dropout')

model_13.summary()

model_13.compile(loss='mse', 
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['mae', 'mse'])

history_13 = model_13.fit(train_char_dataset, 
                        epochs=100,
                        steps_per_epoch = len(train_char_dataset),
                        validation_data=val_char_dataset,
                        validation_steps = len(val_char_dataset),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint(model_13.name)])

model_13.save("/content/drive/MyDrive/Novozyme_Competition/novozyme_competition/model_13_biLSTM_two_input_with_regularization")

model_13.evaluate(val_char_dataset)

model_13_preds = model_13.predict(val_char_dataset)
model_13_preds[:20]

model_13_test_preds = model_13.predict([test_char_seq, test_ph_norm])
model_13_test_preds[:30]

"""## GRU Model"""

# 1. Setup protein sequence inputs
sequence_inputs = layers.Input(shape=(1), dtype=tf.string, name='sequence_inputs')
sequence_vectors = char_vectorizer(sequence_inputs)
sequence_embeddings = character_embedding(sequence_vectors)
x = layers.GRU(64, return_sequences=True)(sequence_embeddings)
sequence_outputs = layers.GRU(64)(x)
sequence_model = tf.keras.Model(inputs = sequence_inputs, outputs = sequence_outputs)

# 2. Setup pH inputs/Model
pH_inputs = layers.Input(shape=(1,), dtype=tf.float32, name='pH_inputs')
norm1 = normalizer1(pH_inputs)
pH_outputs = layers.Dense(128, activation='relu')(norm1)
pH_model = tf.keras.Model(inputs = pH_inputs, outputs = pH_outputs)

# Concatenate sequence and pH inputs 
sequence_pH_concat = layers.Concatenate(name='concatenation')([sequence_model.output, 
                                                               pH_model.output])

combined_dense = layers.Dense(128, activation='relu')(sequence_pH_concat)
output_layer = layers.Dense(1, activation='linear')(combined_dense) 

# Construct the model 
model_14 = tf.keras.Model(inputs=[sequence_model.input, pH_model.input],
                         outputs=output_layer,
                         name='model_14_GRU')

model_14.compile(loss='mse',
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['mae', 'mse'])

history_14 = model_4.fit(train_char_dataset, 
                        epochs=5,
                        steps_per_epoch = len(train_char_dataset),
                        validation_data=val_char_dataset,
                        validation_steps = int(0.25*len(val_char_dataset)),
                        callbacks=[reduce_lr, early_stopping, model_checkpoint(model_14.name)]))

# Function to calculate spearman correlation coefficient
def spearman_coef(model, true_targets, test_dataset):
  """
  Function to calcualte the spearman_coef of a trained model's results

  Args
  --------
  model :  a trained Tensorflow model 

  true_targets : 

  Returns
  --------
  the spearmanr coefficient for the model's predictions and the true_targets

  """
  # Make predictions on test sequences and pH
  pred_targets = model.predict(test_dataset)

  return spearmanr(pred_targets, true_targets)

test_df.head()

import wandb

wandb.init(project='novoenyme_competition')
# # 2. Save model inputs and hyperparameters
config = wandb.config
config.learning_rate = 0.01
# Model training here 
# 3. Log metrics over time to visualize performance
# with tf.Session() as sess:
# # ...
# #wandb.tensorflow.log(tf.summary.merge_all())

